



<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#FFF">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">


<link rel="alternate" type="application/rss+xml" title="FP的一亩三分地" href="https://fouforpast.github.io/rss.xml" />
<link rel="alternate" type="application/atom+xml" title="FP的一亩三分地" href="https://fouforpast.github.io/atom.xml" />
<link rel="alternate" type="application/json" title="FP的一亩三分地" href="https://fouforpast.github.io/feed.json" />

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="/css/app.css?v=0.2.5">

  
  <meta name="keywords" content="遥感,多模态,论文阅读" />


<link rel="canonical" href="https://fouforpast.github.io/2022/10/29/%E9%81%A5%E6%84%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%BC%E8%BF%B0/">



  <title>
遥感多模态综述 - 论文阅读 |
Foufor Past = FP 的一亩三分地</title>
<meta name="generator" content="Hexo 7.0.0"></head>
<body itemscope itemtype="http://schema.org/WebPage">
  <div id="loading">
    <div class="cat">
      <div class="body"></div>
      <div class="head">
        <div class="face"></div>
      </div>
      <div class="foot">
        <div class="tummy-end"></div>
        <div class="bottom"></div>
        <div class="legs left"></div>
        <div class="legs right"></div>
      </div>
      <div class="paw">
        <div class="hands left"></div>
        <div class="hands right"></div>
      </div>
    </div>
  </div>
  <div id="container">
    <header id="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="inner">
        <div id="brand">
          <div class="pjax">
          
  <h1 itemprop="name headline">遥感多模态综述
  </h1>
  
<div class="meta">
  <span class="item" title="创建时间：2022-10-29 21:11:41">
    <span class="icon">
      <i class="ic i-calendar"></i>
    </span>
    <span class="text">发表于</span>
    <time itemprop="dateCreated datePublished" datetime="2022-10-29T21:11:41+08:00">2022-10-29</time>
  </span>
</div>


          </div>
        </div>
        <nav id="nav">
  <div class="inner">
    <div class="toggle">
      <div class="lines" aria-label="切换导航栏">
        <span class="line"></span>
        <span class="line"></span>
        <span class="line"></span>
      </div>
    </div>
    <ul class="menu">
      <li class="item title"><a href="/" rel="start">Foufor Past</a></li>
    </ul>
    <ul class="right">
      <li class="item theme">
        <i class="ic i-sun"></i>
      </li>
      <li class="item search">
        <i class="ic i-search"></i>
      </li>
    </ul>
  </div>
</nav>

      </div>
      <div id="imgs" class="pjax">
        <ul>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1gicitcxhpij20zk0m8hdt.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclhfehz7j20zk0m8u0x.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giciusoyjnj219g0u0x56.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclhpw3lwj20zk0m8gvw.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclfdu6exj20zk0m87hw.jpg"></li>
          <li class="item" data-background-image="https://tva4.sinaimg.cn/large/6833939bly1giclxfdlttj20zk0m8npd.jpg"></li>
        </ul>
      </div>
    </header>
    <div id="waves">
      <svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
        <defs>
          <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z" />
        </defs>
        <g class="parallax">
          <use xlink:href="#gentle-wave" x="48" y="0" />
          <use xlink:href="#gentle-wave" x="48" y="3" />
          <use xlink:href="#gentle-wave" x="48" y="5" />
          <use xlink:href="#gentle-wave" x="48" y="7" />
        </g>
      </svg>
    </div>
    <main>
      <div class="inner">
        <div id="main" class="pjax">
          
  <div class="article wrap">
    
<div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList">
<i class="ic i-home"></i>
<span><a href="/">首页</a></span><i class="ic i-angle-right"></i>
<span  class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="item" rel="index" title="分类于 论文阅读"><span itemprop="name">论文阅读</span></a>
<meta itemprop="position" content="1" /></span>
</div>

    <article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN">
  <link itemprop="mainEntityOfPage" href="https://fouforpast.github.io/2022/10/29/%E9%81%A5%E6%84%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%BC%E8%BF%B0/">

  <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="image" content="/images/avatar.jpg">
    <meta itemprop="name" content="Foufor Past">
    <meta itemprop="description" content=", ">
  </span>

  <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="FP 的一亩三分地">
  </span>

  <div class="body md" itemprop="articleBody">
    

    <p>From Single- to Multi-modal Remote Sensing Imagery Interpretation: A Survey and Taxonomy </p>
<p>模态是信息的来源或形式。通过各种模态信息，人类可以从多个角度感知世界。同时，遥感观测是多模态的。我们通过全色、激光雷达和其他模态传感器宏观地观察世界。多模态遥感观测已成为一个活跃的领域，它有利于城市规划、监测和其他应用。尽管在这一领域取得了许多进展，但仍然没有一项全面的评估，能够为系统的概览提供统一的评价。因此，在本文中，我们首先强调了单模态和多模态遥感影像判读之间的关键差异，然后利用这些差异来指导我们对级联结构中多模态遥感影像判读的研究。最后，对未来可能的研究方向进行了探讨和展望。我们希望这项调查将成为研究人员回顾最新发展和开展多模式研究的起点。<br><span id="more"></span></p>
<h1 id="From-Single-to-Multi-modal-Remote-Sensing-Imagery-Interpretation-A-Survey-and-Taxonomy"><a href="#From-Single-to-Multi-modal-Remote-Sensing-Imagery-Interpretation-A-Survey-and-Taxonomy" class="headerlink" title="From Single- to Multi-modal Remote Sensing Imagery Interpretation: A Survey and Taxonomy"></a>From Single- to Multi-modal Remote Sensing Imagery Interpretation: A Survey and Taxonomy</h1><p>本文是遥感领域多模态解译的一篇综述论文。最近在看多模态相关的内容，粗糙地翻译了一下这篇论文。推荐配合原文阅读。</p>
<p>原文指路：<span class="exturl" data-url="aHR0cHM6Ly9lbmdpbmUuc2NpY2hpbmEuY29tL2RvaS8xMC4xMDA3L3MxMTQzMi0wMjItMzU4OC0w">https://engine.scichina.com/doi/10.1007/s11432-022-3588-0</span></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>模态是信息的来源或形式。通过各种模态信息，人类可以从多个角度感知世界。同时，遥感观测是多模态的。我们通过全色、激光雷达和其他模态传感器宏观地观察世界。多模态遥感观测已成为一个活跃的领域，它有利于城市规划、监测和其他应用。尽管在这一领域取得了许多进展，但仍然没有一项全面的评估，能够为系统的概览提供统一的评价。因此，在本文中，我们首先强调了单模态和多模态遥感影像判读之间的关键差异，然后利用这些差异来指导我们对级联结构中多模态遥感影像判读的研究。最后，对未来可能的研究方向进行了探讨和展望。我们希望这项调查将成为研究人员回顾最新发展和开展多模式研究的起点。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>装备各种传感器的高空间分辨率（HSR）卫星的发展，带来了丰富的数据来源。得益于此，HSR遥感图像具有多模态特性，这为遥感和计算机视觉界提出了更具挑战性的问题。与单模观测相比，它提供了更多关于传感器、角度、分辨率和时间的信息，这是至关重要的，为前沿带来了巨大的推动[1-4]。因此，如何充分利用多模态遥感影像进行地球观测就显得尤为迫切。相关研究进一步表明遥感影像解译正逐步由单一模型向多模型发展，提供了更多的观测细节来细化场景信息。因此，本文对多模态遥感解译的前沿技术进行了全面、及时的综述，以帮助研究人员学习和应用多模态遥感解译的前沿技术，为进一步的实验奠定坚实的基础。</p>
<p>多模态遥感图像解译(MRSII)是地球观测和计算机视觉领域的一个新兴方向。它具有挑战性，具有比单模态更大的应用价值。从特性的角度来看，至少有四个原因:</p>
<p>1)<strong>图像数据是多光谱的</strong>。如图1所示，不同传感器的成像机理和光谱带不同。在获取场景空间图像信息的同时，各元素的光谱特征向量反映了场景的地球物理性质。由于上述特性，现有的预训练模型[6-8]在通道尺寸和高级空间表示方面面临挑战。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029205750812.png" alt="image-20221029205750812"></p>
<p>2)<strong>同一区域的多模态观测提供了来自不同视角(如三维空间、距离和高度)的互补信息</strong>。对于一些地球观测应用，如城市土地规划、三维重建、森林分类等，我们需要多模态传感器从多个角度观察场景。但由于受计算资源的限制，观测角度越多，计算复杂度越大。</p>
<p>3)表1列出了一些著名卫星的参数。<strong>由于MRSII的多尺度特性，要求系统具有处理不同分辨率图像的自适应能力</strong>。例如，在WV-3图像中，一架飞机可能会占用大约400像素，但在高分2图像中只占用150像素。这种情况给单模态模型带来了巨大的挑战，特别是当同一对象之间存在显著的尺度变化时。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029195348006.png" alt="image-20221029195348006"></p>
<p>4)多模态图像为地球监测提供了<strong>时间维度</strong>，为相关研究提供了新的动力。多时间变化检测、数据融合、基于域适应的分割/检测等一系列具有广阔应用前景的时间任务应运而生，加速了遥感影像解译向多维度、多任务的方向发展。</p>
<p>如图2所示，我们检索了Web of Science 上与MRSII相关的出版物。从近20年的文献变化趋势来看，MRSII的数量逐年增加，MRSII已经成为遥感研究的热点。尽管该领域有超过30年的研究和它在理论和实践层面的重要性，很少有相关的综述可用。这是一个遥感影像解译由单一模式向多模式转变的时代。我们希望我们的工作将有助于遥感和计算机视觉的社区。本文的主要贡献有四个方面：</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029195417736.png" alt="image-20221029195417736"></p>
<p>(1)我们利用多平台、多传感器数据对MRSII的论文进行了全面和及时的综述。通过详尽的阐述，可以把握MRSII的整个发展过程，构建完整的MRSII知识体系。</p>
<p>(2)提出了一种易于理解的层次分类法，将MRSII方法分为不同的任务:多源融合、多模态表示、多源对齐、跨模态翻译和联合学习，然后根据可泛化的特性对每个任务进行更详细的分类。</p>
<p>(3)我们总结了除了普通MRSII外最近出现的几个扩展研究主题，并讨论了这些主题的最新进展。这些课题具有挑战性，同时也为许多现实的影像解译问题的解决提供了突出的现实意义。</p>
<p>4)在总结的基础上，进一步探讨了MRSII的应用和未来发展方向，以期为从事多模态遥感影像解译的研究者提供参考。</p>
<h2 id="2-Taxonomy"><a href="#2-Taxonomy" class="headerlink" title="2. Taxonomy"></a>2. Taxonomy</h2><p>目前，该领域的研究人员对MRSII存在不同意见的根本原因是它涉及的领域更广，边界模糊。不同的观点导致不同的解译和分类结果。在本文中，我们参考[9]并将MRSII方法分为五大类(如图3所示)，即多源对齐(第3节)、多源融合(第4节)、多模态表示(第5节)、跨模态翻译(第6节)和联合学习(第7节)，以应对MRSII挑战的核心技术。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029195503510.png" alt="image-20221029195503510"></p>
<p>1)<strong>Alignment</strong> 多模态对齐提供不同模态信息的对齐和匹配，旨在发现模态之间的空间和时间联系。例如，来自不同传感器的图像之间的图像配准和检索，以及图像与文本之间的检索和匹配。这些方法的重点是将不同的模式映射到统一的语义空间，并通过距离度量它们的相似性</p>
<p>2)<strong>Fusion</strong> MRSII中的多源融合旨在将两个或多个遥感数据或其他观测数据与相同复杂场景的互补信息结合起来。通过结合它们的信息进行处理、分析和决策，可以获得用于目标预测(分类或回归)的更高质量的数据。例如，将高分辨率全色图像与多光谱图像融合，可使多光谱图像的空间分辨率提高数倍。</p>
<p>3)<strong>Representation</strong>  一项基本任务是将图像编码到用于下游任务分析的高级特征空间中。类似地，在MRSII中，representation负责将多模态信息提取并抽象为高级特征向量。它利用不同模态特征之间的互补性，消除冗余，以学习更好的特征。例如，为了对城市分类和三维建筑重建进行编码，将数字表面模型(Digital Surface Model, DSM)和真正射电像相结合，送入相同的表示空间。</p>
<p>4)<strong>Translatioin</strong>  一个新出现的挑战是将信息从一种形式翻译成另一种形式。该任务的方法倾向于生成模型，预测的目标是开放的或主观的。生成的模态与源模态是异构的。例如，我们使用SAR数据生成全色图像。</p>
<p>5)<strong>Co-learning</strong>  对于一些复杂的场景，一个单模态传感器可能是不足的，因此需要另一个丰富的模态来辅助它的学习。在某些需要域适应或迁移的情况下，跨模态信息可以利用联合学习来辅助学习。例如，利用资源丰富的光学图像特征进行预训练，然后对稀缺的SAR图像特征进行学习，可以提高模型的性能。</p>
<p>为了帮助说明和组织MRSII新兴研究领域的最新工作，我们进一步细分和总结每个分类。对于不同的分类，它们不是不相关的，而且在许多情况下互为补充。它们在各种情况下相互补充，一个优秀的多模态模型通常需要组合两种以上的技术。例如，多模态表示可以用作alignment或translation的骨干模型。在接下来的五个部分中，我们将详细解释这些任务。</p>
<h2 id="3-Multi-source-Alignment"><a href="#3-Multi-source-Alignment" class="headerlink" title="3. Multi-source Alignment"></a>3. Multi-source Alignment</h2><p><strong>遥感多源对齐的目的是将原始源与目标源进行匹配，在异构数据之间找到相应的显式和隐式关系</strong>。例如，给定两张来自不同传感器的包含相同复杂场景的图像，我们将匹配或检索它们的子组件（sub-components）。多源对齐是MRSII的一个重要分支，相关工作包括图像配准[12-14]、变化检测[18,27 - 29]和跨模态检索[26,30,31]。</p>
<p>如图8所示，根据数据源的对齐维度，我们将多源对齐方法分为三种:1)空间对齐，2)时间对齐，3)交叉元素对齐。表2列出了这些方法的不同。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029200344039.png" alt="image-20221029200344039"></p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029205246902.png" alt="image-20221029205246902"></p>
<h4 id="3-1-Spatial-Alignment"><a href="#3-1-Spatial-Alignment" class="headerlink" title="3.1 Spatial Alignment"></a>3.1 Spatial Alignment</h4><p>空间对齐主要是图像对齐的过程。即找到来自同一复杂场景的当前图像与参考图像之间的像素空间映射关系，从而实现不同图像源的几何同步。这些图像通常由不同的传感器在不同的时间和视点拍摄[10,11]。空间对齐是一项重要的任务，它会显著影响MRSII的预处理步骤、图像融合、图像拼接和地图更新等。</p>
<p>在过去的几十年里，遥感领域发展了许多类型的空间对准技术。根据训练样本类型的不同，空间对齐框架可分为三种类型:无监督方法、半监督方法和监督方法。</p>
<p><strong>Unsupervised methods</strong>没有任何事先训练的样本，需要直接对数据建模。它们是第一批应用于多模态对齐的方法，通过构建一系列范式并从这些范式中识别它们潜在的类规则来聚类同一类特征。</p>
<p>最初，无监督方法的应用方向是图像配准，将不同传感器在不同时间捕捉到的同一场景的两张或多张图像对齐，是各种遥感应用的重要前提[32,33]。在[34-36]中，作者采用互信息最大化算法，并结合其他特征增强方法，对不同模态卫星数据进行精确配准。自然，无监督空间对齐在土地覆盖分类中也有广泛的应用[15-17]。</p>
<p><strong>Semi-supervised method</strong>是另一种空间对齐框架，它利用大量的未标记和标记数据来执行MRSII[37,38]。在[39]中，MAPPER被用来进行光学数据和偏振SAR数据的多重对齐，用于土地覆盖和当地气候的半监督分类。半监督研究利用半监督对齐方法从潜在空间获取具有多时间、多源、多传感器和多角度特征的图像的线性可逆变换。Hong[40]提出了一种可学习流形对齐框架，直接从数据中学习joint graph structure。采用半监督学习方法对多模态图像进行对齐，可以减少标注人员的工作量，获得较高的对齐精度。因此，它受到了遥感界的广泛关注。</p>
<p><strong>Supervised methods</strong>根据来自标记数据集的输入和输出结果之间的关系训练一个最佳模型。在监督学习中，训练数据既有特征又有标签，通过训练，机器可以自己找到特征和标签之间的联系。</p>
<p>由于数据量大，一些方法[41-43]使用非深度学习架构。随着深度学习的发展，监督方法已经成为空间对齐的主流。在[44-46]中，作者设计了生成网络来生成耦合的光学和SAR图像，并使用深度匹配网络进行匹配。Zhang等和Fan等[47,48]提出了一种用于多模态图像配准的孪生神经网络，该网络采用了使正和难负样本（hard negative samples）之间的特征距离最大化的策略。</p>
<h4 id="3-2-Temporal-Alignment"><a href="#3-2-Temporal-Alignment" class="headerlink" title="3.2 Temporal Alignment"></a>3.2 Temporal Alignment</h4><p>时间比对主要针对长序列遥感影像分析。与空间对齐相比，它负责寻找来自同一子实例的不同模态信息的子分支或元素之间的对应关系。如图8所示，给定来自不同传感器的系列图像，时间对齐面向场景中随时间变化的实例，并可进一步用于下游任务，如区域规划、作物、植物分类等。因此，时间对齐对算法对时间相关性和空间变化的敏感性提出了挑战。</p>
<p>目前的时间对齐主要是针对一对多模态图像之间的元素对齐。根据网络结构的对称性将方法分为:<strong>对称结构、非对称结构</strong>。如图4对称结构中所示，不同数据源的子网络结构是相同的体系结构，各模态特征之间存在交互作用。在非对称结构中，网络结构是不对称的，每一边的编码器层和投影层都不同。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029200226052.png" alt="image-20221029200226052"></p>
<p><strong>Symmetric structures</strong>更倾向于通过网络结构学习和匹配不同模态信息之间的属性。在[50]中，提出了一种基于图的数据融合算法，用于数据驱动的半无监督变化检测和水稻作物生物量估计。Sun等人[19-21]构造了一个鲁棒的k近邻图来学习每个图像的结构，并使用图映射来比较同一图像域中的图。Yang等[51]提出了一种用于异构图像变化检测的深度金字塔特征学习网络</p>
<p><strong>Asymmetric structures</strong>更强调不同模态信息之间的特征，用不同的编码器学习各种模态特征，然后用解码器进行融合解码[52-54]。</p>
<h4 id="3-3-Cross-element-Alignment"><a href="#3-3-Cross-element-Alignment" class="headerlink" title="3.3 Cross-element Alignment"></a>3.3 Cross-element Alignment</h4><p>人工智能的日益成熟给遥感领域带来了更多新的机遇和挑战。MRSII融合了越来越多的新元素，如语音、文本、OSM和其他非遥感模态。跨元素对齐旨在实现遥感图像和非遥感模态之间的全局或子组件对齐。通过调整模态，它可以进一步用于图像检索和视觉问题回答任务。</p>
<p>根据跨元素对齐的目的，我们将其分为两类:场景增强和人机交互。第一类是融合和对齐非观测模态，<strong>以减少观测误差，获得更全面和准确的地表数据</strong>。第二类是<strong>通过将其他模式与图像对齐，实现协同检索，提高图像检索速度</strong>，从而更好地方便人员查询和搜索。</p>
<p>有许多非观测特征可以提供遥感场景的增强表示。它们具有与空间对齐相同的目的，在模态中匹配和对齐相同区域，以用于下游任务。在[55-58]中，作者通过寻找实体之间的最佳匹配，将Openstreetmap与遥感图像相匹配，用于building footprint的描述、更新和城市土地利用制图。并且[59-62]结合了生物量、植被覆盖和全球电离层图估计的地基数据，显著提高了单模态估计的准确性和置信度。此外，许多研究人员将GNSS[63-65]、GIS[66-68]、水文气象学[69-71]和其他信息进行对齐，以实现在交通统计、地图绘制、动物行为、环境相互作用等方面的应用。</p>
<p>为了更好的人机交互，研究人员将语音和文本模式与遥感图像相结合。在[31,72-74]中，作者讨论了基于图像和语音的遥感标签标注之间的多标签跨模态信息检索问题，通过学习输入模态的判别共享特征空间的深度神经网络体系结构，适合于语义一致的信息检索。[24-26]设计了一系列图像-文本匹配网络，以探索遥感图像与其各自自然语言描述之间的相关性。</p>
<h4 id="3-4-Related-work-and-Challenges"><a href="#3-4-Related-work-and-Challenges" class="headerlink" title="3.4 Related work and Challenges"></a>3.4 Related work and Challenges</h4><p>广义模态对齐更侧重于跨元素对齐，检索和匹配两个或两个以上模态[9]之间实例关系的子组件，如图像+文本[75-77]、视频+音频[78-80]、视频+文本[81-83]等。</p>
<p>除了不同元素的对齐之外，模态对齐在MRSII中更关注各种传感器的对齐。在多源对齐中，仍然存在以下挑战:1)<strong>图像规模过大，包含的实例子组件数量远远大于自然场景</strong>。2)<strong>相关数据集中的数据量太小</strong>，使得有监督模型难以进行图像对齐检索，在训练过程中容易出现过拟合问题。3)<strong>实例的子组件复杂，形状和方向任意</strong>，即使是同一区域的实例也会因为成像而产生失真或缺失。</p>
<h2 id="4-Muti-source-Fusion"><a href="#4-Muti-source-Fusion" class="headerlink" title="4. Muti-source Fusion"></a>4. Muti-source Fusion</h2><p>受传感器成像机理的限制，遥感影像的空间分辨率和光谱分辨率是相互制约的，单一成像手段无法获得高空间分辨率和高光谱分辨率的遥感影像。多源融合是在遥感数据指标上解决传感器瓶颈限制的有效途径。它通过算法组合来自不同指标或来源的数据，获得比单一数据源更丰富的信息。</p>
<p>多源数据融合作为MRSII的重要组成部分，有着悠久的历史。多源数据融合的概念发展于20世纪70年代初，但理论方法直到20世纪90年代被提出。近年来，它的发展迅速，仍然是一个热门的研究课题。多源融合的应用非常广泛，包括自然资源调查[94-96]、精准农业[97-99]、城市规划[100-102]等。在本节中，我们将通过融合级别和类别详细回顾多源融合。然后根据融合的类型来进行分类。</p>
<p>参考D.L. Hall等人[103]，我们将多源数据融合细分为三个级别:1)数据级融合，2)特征级融合，3)决策级融合。三种体系结构的概述如图5所示。数据级融合是<strong>对原始传感器数据或预处理数据的直接计算处理</strong>，这些数据可以包含数据源最原始的细节。主要目的是提高数据的质量，即分辨率、对比度、完整性和其他指标。特征级融合是在<strong>从目标场景(原始传感器数据)提取特征信息之后</strong>的步骤进行的。它融合提取的特征，生成新的特征，用于后续复杂场景的解释。决策级融合需要<strong>从源图像中提取目标特征，并对特征进行滤波和分类，最后根据特征的类别进行融合</strong>。它主要解决不同数据的决策结果不一致的问题，从而从各种传感器数据中获得更可靠的决策知识。这三种融合策略并非互不相容，而是可以联合使用，多层级融合是一个前沿的研究方向。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029201304729.png" alt="image-20221029201304729"></p>
<p>根据融合数据的类型，我们引入了一种更直接、更明确的分类策略，如表3所示。我们将遥感多源融合分为三类:同质数据融合、异构数据融合和遥感与其他类型数据融合。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029205354688.png" alt="image-20221029205354688"></p>
<h4 id="4-1-Homogeneous-Data-Fusion"><a href="#4-1-Homogeneous-Data-Fusion" class="headerlink" title="4.1 Homogeneous Data Fusion"></a>4.1 Homogeneous Data Fusion</h4><p>同质数据融合是指<strong>来自相同成像方式的传感器之间的数据融合</strong>，如高分辨率全色图像和多光谱图像之间的数据级融合。这种方法的主要目的是提高图像的分辨率，并减轻空间、光谱和时间分辨率之间的相互（制约）联系。同时，通过数据级融合，对图像中的阴影、云层等噪声进行修复和滤波，获得最佳的时间、空间和光谱分辨率。除了全色-多光谱融合[104-106]外，还包括同模态融合[107-109]、全色-高光谱融合[110-112]、多光谱-高光谱融合[113-115]等。</p>
<p>同质数据融合是一个历史悠久的问题，我们将其分为基于空间和基于时空两种方向，融合方法示意图如图6所示。基于空间的方法通过<strong>对图像进行空间对齐，聚焦于空间上一致的图像对，建立特征关系，实现数据融合</strong>。基于时空的融合方法更侧重于<strong>从具有多时相的低分辨率数据推断出特定时间的高分辨率数据</strong>。该算法利用一系列时间图像构建时间和空间维度关系，利用优化约束算法实现融合。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029201541614.png" alt="image-20221029201541614"></p>
<h5 id="4-1-1-Spatial-reference"><a href="#4-1-1-Spatial-reference" class="headerlink" title="4.1.1 Spatial reference"></a>4.1.1 Spatial reference</h5><p>我们确定了三种用于空间参考的算法:<strong>全色锐化</strong>、<strong>线性优化</strong>和<strong>基于深度学习</strong>的算法。全色锐化是一种通过融合全色和多光谱图像来获得高空间和光谱分辨率图像的辐射变换。线性优化主要是通过添加线性约束来实现同模态融合，从而获得最优解的重建图像。基于深度学习的算法通过模拟生物神经元的结构来建模图像之间的非线性关系，从而实现同构数据融合。线性优化主要是通过添加线性约束来实现同模态融合，从而获得最优解的重建图像。基于深度学习的算法通过模拟生物神经元的结构来建模图像之间的非线性关系，从而实现同构数据融合。</p>
<p><strong>Panchromatic sharpening</strong>  常用的全色锐化方法可分为两大类:<strong>成分替换</strong>和<strong>多解析度分析</strong>。成分替换模型将图像投影到新变换的空间中，用高空间分辨率的图像替换包含空间信息的成分，并将将其反变换到原空间，得到空间增强的数据。多解析度分析模型将原始数据分解成不同分辨率的图像进行融合，最后进行反变换得到融合后的图像。</p>
<p>成分替换法(CS)的先驱是IHS变换[116-119]和主成分分析(PCA)[120-123]。IHS变换被广泛用于融合图像，因为它能够分离出RGB图像中的H和S分量中的光谱信息，同时分离出I分量中的大部分空间信息。PCA主要是通过线性变换将数据投影到新变换的空间中，第一主成分沿方差最大的方向变换，保留了原始数据的大部分信息，因此第一主成分进行替换。这些方法的另一种替代方法是Gram Schmidt方法(GS) [124-126]。该方法的本质是Gram-Schmidt正交法，它将酉空间中的一组线性无关向量变换为一组正交向量。此外，还有Brovey变换(BT)[127, 128]，张量因子分解[129,130]等。</p>
<p>由于CS方法能够以较低的计算成本有效地提高空间分辨率，目前仍是研究的重点。基于GS自适应(GSA)、广义IHS (GIHS)[131]、GIHS自适应的方法(GIHSA)[124]和基于比值图像的光谱重采样(RIBSR)[132]已经被广泛研究，减弱了多光谱融合过程中的光谱失真问题。</p>
<p>多解析度分析(MRA)将多模态数据分解成多个分量，当这些分量被重新组合在一起时，就会生成更高分辨率的图像。对图像进行分解和融合以获得更高分辨率的图像是MRA的核心。每个组成部分理想地将图像分解为物理上有意义和可解释的部分。</p>
<p>常用的MRA方法包括高通滤波法(HPF)[120,133]、小波变换[84,85,134]、拉普拉斯金字塔法[86,135]和曲波变换法[136-138]。MRA方法比CS方法能更好地保持光谱信息。但是，如果多模态数据没有严格对齐，在这种情况下，在高频谱细节注入（high-pass detail injection）存在的情况下，融合产品可能会发生空间失真，这通常是由振铃或混叠效应、原始偏移、轮廓和纹理模糊引起的[124]。</p>
<p><strong>Linear optimization</strong>  线性优化模型将数据融合问题归结为其线性最优解，假设多源数据之间的关系为$X_1,X_2,\cdots,X_N$与融合数据$Z$呈线性关系，可表示为</p>
<script type="math/tex; mode=display">
Z=W_1X_1+\cdots+W_NX_N+b</script><p>其中$W_n$是转换因子，$b$是偏差。</p>
<p>根据求解的原理，可分为光谱分解法、贝叶斯概率法和稀疏分解法（<strong>spectral demixing, Bayesian probabilistic, and sparse demixing methods</strong>）。光谱分解将混合像素分解为一系列的组成光谱(端元，end-members)和一组对应的分量(丰度，abundances)，并进行线性求和和重构。贝叶斯概率理论将待融合的数据视为观测值，将融合的数据视为未观察到的真值。它通过计算真实值在给定观测值的情况下出现的概率来求解融合过程中的参数值。贝叶斯概率理论将待融合的数据视为观测值，将融合的数据视为未观察到的真值。该算法通过在观测值下计算真实值出现的概率并使概率最大化来求解融合过程中的参数值。稀疏分解方法将多模态数据分解为字典矩阵（dictionary matrix）和稀疏系数矩阵，并添加稀疏约束对稀疏系数进行求解，得到融合后的数据。</p>
<p><strong>Deep learning-based</strong>  基于深度学习的算法专注于构建同一区域内不同图像之间的非线性关系。最常见的方法是基于卷积神经网络(CNN)，它通过放弃全局连通性来解决权重数量极其庞大的问题[139-141]。有两个代表性的工作，Scarpa等人[142]设计了轻量级CNN和目标自适应使用方式，以确保在数据源不匹配的情况下也能获得良好的性能。在[115]中，作者提出了一种3D-CNN来融合MS和HS图像，以获得高分辨率的高光谱图像。</p>
<h5 id="4-1-2-Spatio-temporal-reference"><a href="#4-1-2-Spatio-temporal-reference" class="headerlink" title="4.1.2 Spatio-temporal reference"></a>4.1.2 Spatio-temporal reference</h5><p>基于时空的方法与基于空间的方法在空间关系构建中的方法本质上是相同的。因此，我们将研究重点放在时间关系的建构上。</p>
<p>早期的工作集中在线性优化模型。时空自适应反射率融合模型(STARFM)是Gao等人[143]提出的一种预测时空融合的有效方法。基于STARFM，一系列时空参考算法陆续被提出，如ESTARFM [144]，STRUM[145]，USTARFM[146]等。Xue等人[147]提出了一种组合时间序列中的时间相关信息的贝叶斯统计算法，他们将融合问题视为具有最大后验(MAP)估计量的估计问题来获取融合图像。在基于深度学习的算法中，主要是寻找空间中的非线性关系，构建时间关系相对较少。</p>
<h4 id="4-2-Heterogeneous-Data-Fusion"><a href="#4-2-Heterogeneous-Data-Fusion" class="headerlink" title="4.2 Heterogeneous Data Fusion"></a>4.2 Heterogeneous Data Fusion</h4><p>异构遥感数据融合是指<strong>来自不同成像方式的传感器之间的融合</strong>，如光学-雷达、SAR-多光谱、SAR-高光谱数据融合等。由于不同传感器之间的成像机制差异过大，异构数据融合<strong>更适合于特征级、决策级的融合，如特征分类、变化检测、参数反演等</strong>。</p>
<p>根据优化方法的不同，我们将其分为基于特征堆叠的方法、基于子空间的方法和基于深度学习的方法。在基于特征堆叠的方法中，我们将辅助传感器提取的信息叠加到图像的每个像素上，得到包含所有模态信息的特征向量。基于子空间的方法将所有信息投射到一个低维子空间中，然后进行特征融合。基于深度学习的方法学习系统输入和输出之间的非线性关系。这些方法能够较好地刻画不同分辨率图像之间的非线性关系，具有很强的可移植性。</p>
<p><strong>Feature stacking-based</strong>  基于特征堆叠的方法是异构融合最简洁的实现。该策略在相同的结构中过滤和堆叠各种源数据。例如，将从LiDAR数据中提取的高度和强度特征叠加到多/高光谱图像的光谱波段中，并对复杂场景中的每个像素形成扩展的特征向量[148]。</p>
<p>形态学轮廓、属性轮廓和消光轮廓被广泛应用于特征提取和过滤，以充分利用异构数据中有辨识度的特征信息。这些方法[149-151]概念简单，计算效率高，通常用于异构数据融合，并提供高质量的融合结果。</p>
<p>虽然基于特征叠加的方法可以获得更好的融合结果，但从异构数据中提取的光谱、空间和海拔特征的直接叠加增加了样本的特征维数，从而给后续的分类任务带来了两个主要挑战:维数灾难和高计算复杂度。</p>
<p><strong>Subspace-based</strong>   基于子空间的方法避免了后续分类任务的维数灾难，提高了计算效率。它们将异构数据中的特征表示为低维子空间中的特征，以减轻后续任务的压力。在原始的子空间模型中，子空间的基准和融合特征都是未知的，如何估计它们是子空间模型的核心问题。</p>
<p>许多基于子空间的方法的早期工作使用了经典的IHS变换[152]或PCA方法[153,154]。这些方法可以有效降低特征维数，提高信噪比，减少计算量，提高异构数据融合问题的分类精度。</p>
<p><strong>Deep learning-based</strong>  遥感场景通常具有复杂的类别分布，导致遥感数据与目标样本之间存在非线性关系。多传感器数据融合增强了这种非线性关系，使样本在特征空间中表现出高阶非线性。基于深度学习的方法可以很好地拟合异构数据之间的非线性关系，并具有从数据中提取高阶、多维、抽象特征的能力。深度学习提取的特征一般不受样本的非线性分布的影响，对复杂场景具有鲁棒性。</p>
<p>基于深度学习的方法[87-89]可以获得更好的融合结果和分类精度。但由于通常需要大量的标记样本进行训练，且遥感场景的标记样本通常难以获取，这在一定程度上限制了深度学习方法在异构数据融合中的应用。</p>
<h4 id="4-3-Remote-Sensing-And-Other-Type-Data-Fusion"><a href="#4-3-Remote-Sensing-And-Other-Type-Data-Fusion" class="headerlink" title="4.3 Remote Sensing And Other Type Data Fusion"></a>4.3 Remote Sensing And Other Type Data Fusion</h4><p>遥感数据还可以与其他类型的数据融合，进行处理和协同应用，从而获得更多关于资源环境特征的数据。将遥感数据与全景采集数据、景观图像以及陆地、大气、水文等数据进行融合，为大规模复杂场景的数据感知提供了更精确的观测。遥感数据可以为场景提供更精确的初始观测和边界条件，然后自动连续地将它们和其他数据进行调整，从而将模拟误差降低到高精度、空间连续的地表数据。这种融合方法是当前发展的一个重要趋势。</p>
<p>遥感和地面观测是获取对地观测数据的两种重要途径。遥感可以提供大尺度的区域观测，但由于其成像过程复杂，易受环境干扰，观测精度往往难以保证。地面观测质量高，但观测点稀疏，难以获得全面的观测。因此，遥感与地面观测数据的融合引起了众多研究者的关注[90-93]。同时，遥感数据与大气数据、流体动力数据的融合可以进一步降低模拟误差，可用于水文气象[155,156]、植被[157,158]和大气信息[159,160]的协同分析。</p>
<h4 id="4-4-Remaining-problems"><a href="#4-4-Remaining-problems" class="headerlink" title="4.4 Remaining problems"></a>4.4 Remaining problems</h4><p>在多模态融合中，同构数据融合技术已经非常成熟，在生活中有着广泛的应用。我们在互联网上获得的光学图像是同构融合数据。而在异构融合和其他类型的数据融合中，由于模型对不同传感器和场景的适应性较差，尽管融合后的图像在空间维度和可视化方面比单模态遥感图像有很大的提高，但是在应用中仍然需要根据场景环境进行测试和选择不同的模型。为此，我们重点研究了异构数据融合和其他类型数据融合中存在的问题，希望能给研究者带来一些启示，具体如下:</p>
<p>1)<strong>数据来源不同</strong>。不同传感器所携带的模态信息存在较大的类间差异。融合过程需要进行信息之间地理空间上的对齐和标准化处理，去除多模态数据的冗余性，即在保留有效信息的同时去除冗余信息。</p>
<p>2)<strong>观察角度不同</strong>。不同模态数据的观察角度是不同的。卫星对同一区域有不同的视角，即使进行了正交校正，两幅图像也不可能完全相同。此外，其他类型的数据，如地面数据，由于地面观测平台的原因，在与遥感图像融合时，数据特征很难对齐。</p>
<p>3)<strong>不同分辨率</strong>。多模态融合需要解决不同分辨率的问题，特别是在异构数据的融合中，由于数据类型的巨大差异导致了这一问题更加突出。分辨率的尺度影响模型的性能，当分辨率过高时，模型更倾向于观察小目标(汽车、树木、小屋)，而当分辨率较低时，模型更适合观察大目标(体育场、道路、高层建筑)。</p>
<p>4)<strong>未知观测场景。</strong>遥感场景是不可预测的，现有模型往往只适用于单一场景，如城市、森林、沙漠、海洋等。由于观测场景往往存在未知情况，因此提高模型的鲁棒性以应用于未知的观测场景是未来的发展方向之一。</p>
<h2 id="5-Multimodal-Representation"><a href="#5-Multimodal-Representation" class="headerlink" title="5 Multimodal Representation"></a>5 Multimodal Representation</h2><p><strong>使用机器学习方法将原始数据转换为计算机可以识别和处理的数学表示，进一步提取有用信息，以便用于分类或其他预测任务</strong>，这是表示研究的一个主要领域。在遥感多模态表示中，多模态数据从不同角度描述了复杂的场景，上下文信息是互补的或补充的。因此，它们比单模态数据携带了更多优秀的信息，所以利用来自异构源的多个模态提供的综合语义是有价值的。</p>
<p>机器学习方法的性能在很大程度上取决于应用数据表示特征的有效表示[161]。对于视觉[7,162 - 164]、文本[165-167]、语音[168,169]和图形[170-172]形式，单模态特征的表示相对先进，在现实应用中得到广泛应用。然而，在多模态特征表示中，特别是在遥感场景的多模态特征表示学习中还存在许多难点:1)如何抑制来自不同传感器的不可控噪声，2)如何组合来自异构数据源的小样本数据，3)如何处理不同数据源之间的成像透视图，4)如何解决某些模态下的缺失数据。</p>
<p>受[9]和[173]中定义的启发，为了便于讨论如何清晰有效地表示来自不同模态的数据，我们将遥感多模态表示分为三个框架:1)联合表示，2)协调表示，3)编码器-解码器表示。三种体系结构的概述如图5所示。</p>
<p>最常见的遥感图像表示学习是基于CNN的。这项工作倾向于通过使用Deep CNN模型来迁移学习，即利用在自然场景(如LeNet[174])上进行了预先训练的模型，例如VGGNet [7]， GoogleNet[175]和ResNet[6]，从而获得比从零开始训练更好的性能。随着transformer[176]的兴起，它越来越多地被用于图像表示学习[163,177,178]中，这是遥感图像解译的一个新的研究热点[179-181]。它们可以集成到多模态表示学习中，并与其他模态数据一起进行训练(例如，文本数据的word2vec[165]、Glove[166]和BERT[167]以及wav2vec[168]、PASE[182]和Mockingjay[183])。通过使用来自其他模态的表征学习模型进行训练，可以大大提高多模态表征学习的性能。</p>
<h4 id="5-1-Joint-Representation"><a href="#5-1-Joint-Representation" class="headerlink" title="5.1 Joint Representation"></a>5.1 Joint Representation</h4><p>联合表示旨在<strong>将各种单模态特征投射到一个共享的语义子空间中，以减少模态之间的异质性，挖掘特征之间的互补性，从而学习更好的特征表示</strong>。</p>
<p>相关的算法<strong>将来自不同传感器的图像以及其他模态信息表示为特征向量(张量)，缩小异质性差距，获得互补的特征表示</strong>。Manish Sharma等[184]和Yang等[185]通过学习红外传感器的特性，扩展RGB图像的表征能力，提高了遥感和无人机图像在各种天气条件下的目标检测精度。Flynn等[186]和Oliveira等[187]通过光学图像与红外或热成像图像的联合表示，使用航空视频进行人员检测和随时间的跟踪检测，获得了较高的检测精度。Breckon等人[188]引入了一种实时多模态目标检测算法，该算法结合了来自多个自主平台(地面和空中)部署网络的可见光波段、热波段和雷达图像，自动检测人和车辆。</p>
<p>除了目标检测任务外，多模态联合表示在其他遥感影像解译任务中也有广泛的研究应用。对于遥感分类任务，Audebert等人[189,190]研究了激光雷达和多光谱数据的早期和晚期联合表示，发现早期融合允许更好的联合特征学习，但代价是对缺失源的灵敏度更高，而晚期融合使得从模糊源恢复错误成为可能。Li等人[191]提出了一种多模态双线性融合网络来提取光学和SAR图像的深度语义特征图，并对联合表示进行双线性集成。Poliyapram等[2]提出了一种基于深度学习的端到端点式激光雷达和光学图像多模态融合网络，通过整合航空图像特征对航空点云进行三维分割。Jeong等人[192]提出了一种基于多模态传感器的语义三维映射系统，该系统使用三维激光雷达与光学相机相结合的数据。</p>
<p>多分辨率同构数据的联合特征学习也是联合表示的一个重要研究方向。在不同分辨率的图像中，同一物体具有不同的尺度和感知场，并且由于不同传感器的成像方法不同，同一物体内存在色差，这对模型的适应性和鲁棒性提出了更大的挑战。多分辨率联合学习在作物分类[193,194]、目标识别[195,196]、土地覆盖分类等任务中具有广泛的应用和研究价值。</p>
<h4 id="5-2-Coordinated-Representation"><a href="#5-2-Coordinated-Representation" class="headerlink" title="5.2 Coordinated Representation"></a>5.2 Coordinated Representation</h4><p>另一种多模态表示是协调表示。在<strong>协调表示框架中，每个模态单独学习其单独的表示，然后通过统一的约束来协调它们。这类算法更强调不同模态元素的相似性和互补性。它试图在协调子空间中学习每个模态的独立但有约束的表示</strong>。</p>
<p>根据协调表示的目的，我们将这些方法分为两类:互补方法和相似方法。互补方法主要关注模态之间的差异和互补信息，通过比较差异信息来补充和增强复杂场景信息的表示。相似度法更关注不同模态之间的相似度，期望同一语义相关模态之间的距离尽可能小，不同语义之间的距离尽可能大。</p>
<p><strong>Complementary methods</strong>   互补方法<strong>使得协调空间能够发现多模态变异性，以补充融合表示</strong>。例如，提取多光谱图像和Lidar特征，在更高维度上拼接和相互作用，获得互补的融合特征，用于土地覆盖分类[88]。在[1]中，作者将来自谷歌地图的俯视图和来自谷歌街景的每个城市对象的地面图像(侧视图)结合起来，以获得与城市对象相关的互补视觉信息，以增强对城市土地利用的理解。</p>
<p><strong>Similarity methods</strong>   除了学习互补性外，<strong>利用相似度方法协调学习子空间中各模态相同元素的相似度</strong>也是协调表示的一个重要分支。Ye等人[41,200]通过基于SAR图像与激光雷达数据的全局和局部特征的特征表示进行相似性度量来进行图像配准。在[201]中，Uss等人训练了一个双通道的patch matching CNN检测图像块之间的相似性，并测量它们的相互位移。通过对真实遥感图像的测试，该模型具有较高的识别力和较高的定位精度。Zhu等[202]的一种基于深度学习的匹配方法是通过光学和红外图像的比较，在目标图像的搜索窗口中搜索和参考图像中给定点对应的点。</p>
<h4 id="5-3-Encoder-decoder-Representation"><a href="#5-3-Encoder-decoder-Representation" class="headerlink" title="5.3 Encoder-decoder Representation"></a>5.3 Encoder-decoder Representation</h4><p>编码器-解码器表示利用了translation的概念。<strong>它首先通过编码器-解码器体系结构将信息从一个模态转换为另一个模态的特征表示，然后将它们投影到相同的向量子空间，以保持语义的一致性</strong>。例如，给定一幅光学图像，我们的目标是生成相应的SAR特征，或者给定SAR图像生成相应的光学特征。</p>
<p>该方法主要用于<strong>某模态的数据比较复杂、有噪声、数据量小或缺失的情况</strong>。大多数多光谱图像都受到云的影响，利用SAR图像生成相应的光学特征来恢复受影响区域是当前多模态表示学习中的一个热点[203-205]。在<code>/cite gao2020cloud</code>中，为了重建损坏区域，必须建立生成对抗网络将SAR图像与模拟光学图像相结合。Dai等[206]研究了多时相图像，实现了自训练和门控卷积层，以区分浑浊像素和干净像素，弥补了普通卷积层区分能力的不足。</p>
<p>除了消除干扰外，编码器-解码器表示还可以应用于土地覆盖分类、模态变换、目标检测等。在[207]中，Hong等人进一步改进了土地覆盖分类的性能，分别使用self-GANs模块和mutual GANs模块学习对扰动不敏感的特征表示，并消除多模态之间的差距，以产生更有效和更健壮的信息传递。Liu[208]提出了一种模态转换模型，将稀疏模态的信息转化为丰富模态的特征空间，为多时相图像解译任务提供了坚实的基础。</p>
<h4 id="5-4-Discussion"><a href="#5-4-Discussion" class="headerlink" title="5.4 Discussion"></a>5.4 Discussion</h4><p>多模态表示学习是一个广泛研究的课题，为其他应用提供了统一的特征表示空间，如模态对齐、模态转换等。</p>
<p>在本节中，我们将其分为联合表示、协调表示和编码器-解码器表示。从结构图(图7)可以看出，联合表示更适合于不同模态数据均衡，推理过程中需要模态交互共同预测的情况。在协调表示中，各模态相互独立但又相互协调，更倾向于评估过程中缺少数据或单模态输入的情况。编码器-解码器表示更关注具有不平衡样本或需要额外模态辅助学习的任务。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029203202447.png" alt="image-20221029203202447"></p>
<h2 id="6-Cross-modal-Translation"><a href="#6-Cross-modal-Translation" class="headerlink" title="6. Cross-modal Translation"></a>6. Cross-modal Translation</h2><p>在MRSII中，将信息从一种模态转换为另一种模态是一个极好的挑战。<strong>由于遥感场景的复杂性和传感器的多变性，遥感跨模态翻译比自然场景的类内(不同图像模态之间)和类间(图像与其他模态之间)翻译更具挑战性</strong>。遥感跨模态翻译是遥感领域的一个新兴课题。随着深度学习算法和计算机硬件的发展，在场景图像翻译[209-212]、遥感图像描述[213-215]等方面都取得了进展。</p>
<p>跨模态翻译是遥感研究中一个新兴的课题，由于其广泛的应用，已经有大量的算法应用于跨模态翻译。同时，根据模态的类内和类间关系，我们将跨模态翻译分为跨传感器翻译和跨元素翻译两个分支，如图9所示。<strong>跨传感器翻译主要是指图像在不同传感器之间的翻译，如全色和多光谱。跨元素翻译是指图像与其他类型模态之间的翻译。</strong></p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029205103346.png" alt="image-20221029205103346"></p>
<h4 id="6-1-Cross-sensor-translation"><a href="#6-1-Cross-sensor-translation" class="headerlink" title="6.1 Cross-sensor translation"></a>6.1 Cross-sensor translation</h4><p>近年来，遥感数据在地球观测和城市规划中发挥了越来越重要的作用。在获得了大量数据的情况下，由于以下三个原因，跨传感器翻译仍然存在很大的挑战：</p>
<p>1)很大一部分数据受到云、雾等大气因素的干扰，这些不可控因素大大降低了遥感图像的利用率，增加了处理和训练的难度。例如，在Landsat ETM+数据[216]中，约35%的陆地区域被云覆盖，而海洋区域的情况更糟。</p>
<p>2)由于传感器成像和（卫星）revisit time的关系，一些模态图像的数据量相对较小。它严重限制了基于深度学习的算法在这一研究领域的应用。</p>
<p>3)由于传感器载体轨迹的影响，在某些区域或场景，特别是特定时间阶段(季节)，可能会出现遥感数据缺失的情况。因此，在遥感中，对特定时期的特定地点进行成像仍然存在困难。</p>
<p>早期的工作关注的是超分辨率重建(SRR)，即从低分辨率(LR)图像中获取超分辨率(SR)图像。目前流行的SRR方法主要是基于传统算法和基于学习的算法。我们将标准算法分为基于插值和基于稀疏的表示方法。虽然基于插值的方法，如双线性或双三次往往生成过于平滑的图像，带有较为明显的人工痕迹，但由于其实现简单，仍然被广泛使用。[217-219]通过引入一系列具有先验知识的优化策略，提高了模型的性能。基于稀疏的方法增强了线性模型从先验知识中恢复高频信息的能力，如小波变换[220]、耦合稀疏自编码器[221]和外部字典[222]，但这些方法计算复杂，需要大量的计算资源。基于学习的模型试图捕捉图像块之间的共现先验（co-occurrence prior）。深度学习是一种基于学习的基本方法。它通过构建端到端神经网络，如CNN[223-225]、GAN[226-228]、注意力网络[229-231]等，学习并拟合LR和HR图像之间的映射关系。由于它的非线性特性，可以在不需要大量计算资源的情况下恢复高频信息。因此，基于深度学习的SRR成为研究热点。</p>
<p>遥感图像在采集过程中受到各种噪声的影响，使得边缘细节模糊，降低了图像质量。因此，需要去噪以获得清晰、高质量的图像。基于多模态信息的遥感图像去噪方法是一个新的热门课题，它将不同模态的无噪声参考图像作为先验知识纳入去噪目标函数[232,233]。此外，研究人员还对自然气候图像的去噪进行了一系列的研究。在这方面，云的去除已经变得越来越复杂。云的存在是造成光学图像信息缺失的主要因素之一，如何通过其他模态图像生成缺失的信息是一个值得关注的问题。Huang等人[203]提出了一种基于稀疏表示的删除方法来恢复缺失的高分辨率信息。随着GAN网络的发展，越来越多的研究人员采用GAN进行云雾去除，并取得了显著的改进，重建图像更加自然和真实[204,205,211]。</p>
<p>很自然，<strong>跨传感器翻译在解决数据稀缺问题时有着广泛的应用</strong>。该领域主要有两个方向:<strong>跨传感器和跨区域</strong>。跨传感器意味着生成不容易通过资源丰富的数据源访问的数据。[234-236]将SAR转换为光学，用于全天候观测，同时简化SAR图像的观测条件。跨区域是指从区域的一种风格生成到区域的另一种风格，以达到数据增强的目的。Ji等人[237]提出了一种基于域适应的GAN的方法进行土地覆盖分类。Peng等人[238]设计了一种用于建筑物提取的新型FDANet。</p>
<h4 id="6-2-Cross-element-Translation"><a href="#6-2-Cross-element-Translation" class="headerlink" title="6.2 Cross-element Translation"></a>6.2 Cross-element Translation</h4><p><strong>将遥感图像翻译成其他模态信息，或利用其他模态信息对遥感图像的语义信息进行配图并总结图像内容</strong>，在跨模态检索[31,72,74]、智能生成[213,239,240]和场景问答等许多领域发挥着重要作用[241 - 243]。跨元素翻译需要模型充分理解复杂的场景，识别出场景的关键组成部分，通过对高层次语义信息的理解和分析，生成标准化、简洁、全面的模态信息来表示场景。</p>
<p>随着计算资源的丰富和数据量的增加，越来越多的研究人员开始将目光投向遥感图像描述。在[244-246]中，作者设计了一系列基于注意机制的图像描述方法。Huang等人[247]从多尺度特征融合的角度考虑了大尺度场景导致特征缺失或遗漏的问题。Wang等人提出了一种新的词句框架[248]，从图像中提取有价值的词，生成格式正确的句子。</p>
<h4 id="6-3-The-challenges-and-differences-from-nature-scenes"><a href="#6-3-The-challenges-and-differences-from-nature-scenes" class="headerlink" title="6.3 The challenges and differences from nature scenes"></a>6.3 The challenges and differences from nature scenes</h4><p>多模态翻译的挑战主要体现在两个方面:遥感数据的复杂性和评价指标。</p>
<p><strong>遥感图像往往具有大尺度、高密度和大纵横比的特点</strong>。在跨模态翻译过程中，经常会出现信息丢失的问题。因此，它包含的信息比自然场景图像多几倍甚至几十倍。特别是在跨元素翻译中，模型很难保证场景中的所有信息都被描述出来。此外，该问题面临的主要挑战是<strong>如何对图像中的关键信息进行过滤和确定，并对其进行精确提取，同时对其进行清晰详细的描述</strong>。</p>
<p>另一个挑战是对模型性能的评估。由于模态翻译是一个生成问题，<strong>很难通过评价指标自动评价模型的生成质量</strong>，甚至利用人工判断也会造成一定程度的主观性。同时，与自然场景不同的是，进行跨传感器的翻译，例如从光学图像到SAR图像或Lidar数据的翻译，需要专业人员进行评估，这进一步增加了评估的难度。</p>
<h2 id="7-Co-learning"><a href="#7-Co-learning" class="headerlink" title="7. Co-learning"></a>7. Co-learning</h2><p>利用资源丰富的数据辅助资源贫乏的数据进行训练是解决小样本学习的有效方法。在多模态机器学习中，帮助一个模态从一个资源丰富的模态过渡到另一个模态，特别是当另一个模态的信息有限或缺乏标记数据、输入有噪声、具有不可靠的标记[9]时。</p>
<p>在本节中，我们关注MRSII中的联合学习，包括迁移学习、联合训练和小样本学习，即使用其他传感器或模态来辅助一种模态进行有效学习。通过使用联合学习从不同的模态信息中学习特征，我们可以获得一个更健壮的模型，作为样本缺乏或样本带有噪声问题的有效解决方案。</p>
<p><strong>Transfer learning</strong>   迁移学习是遥感影像解译中最常用的工具之一。通过<strong>使用在大规模自然场景中训练的预训练模型作为解译模型的骨干</strong>，可以提高模型的收敛速度和性能。跨传感器迁移学习也得到了广泛的研究和应用。2010年，Yao等人[249]引入了MultiSource-TrAdaBoost和TaskTrAdaBoost，用于从多个来源转移知识。Liu等人[250]提出了一种新的域适应方法用于多模态数据的无监督迁移学习——多核联合域匹配。</p>
<p><strong>Co-training and few-shot learning</strong>  联合训练和小样本学习也是联合学习的主要研究领域。在[252]中，Hu等人设计了一种联合训练分类方法来处理不明确的观察样本。Qiu等人[253]结合Sentinel-2和Landsat-8图像，以及Global Urban Footprint、OSM和Nighttime Light数据，研究了它们的相关关系以区分不同LCZ分类。对于小样本学习，Rostami等[3,254]将知识从Electro-Optical domains转移到SAR域，以消除对于大量标记SAR图像的需求。Ying等人[255]提出了一种有效的轻量级CNN，可以有效地将先验知识从光学、混合光学和非光学领域迁移到SAR图像的目标识别任务中。</p>
<p>在MRSII中，联合学习面向<strong>目标数据较少或缺失</strong>的情况，主要体现在数据源缺失和区域缺失两个方面。利用丰富的源数据与目标数据进行辅助或共同学习，可以大幅提高模型的性能，是遥感领域的研究热点。</p>
<h2 id="8-Datasets-of-MRSII"><a href="#8-Datasets-of-MRSII" class="headerlink" title="8. Datasets of MRSII"></a>8. Datasets of MRSII</h2><p>在本节中，我们将讨论MRSII的相关数据集。我们根据数据的维度将不同的数据集分为空间、时间和跨元素(表4中总结了一些代表性的数据集)。</p>
<p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029204228639.png" alt="image-20221029204228639"></p>
<p><strong>Spatial Dataset</strong>  空间数据集中的图像是相互分离的，这些工作的算法更关注基本的计算机视觉任务，例如，分类[6,162,263]，目标检测[264-266]，分割[267-269]，和图像检索[270-272]。随着复杂网络和脑科学的发展，多模态数据集得到了持续的关注。研究人员利用多模式信息，例如，光学/激光雷达[190]，光学/SAR [256]，SAR/Lidar[257]，以解决更复杂的场景理解，这对机器来说是一个极具挑战性的问题。然而，现有的多模态数据集没有足够多的注释良好的数据来支持大多数基于深度学习的技术。当数据量相对于模型参数数量过于稀缺时，容易出现过拟合问题。</p>
<p><strong>Temporal Dataset</strong>  时间数据集更多关注同一区域的时间演化，现有的工作包含两幅图像的比较，关注特定的对象实例。例如，LEVV-CD[258]和HRSCD[259]是变化检测的基本驱动。许多优秀的模型都是基于这些数据集上实现，并在日常生活中得到了应用。CRC[260]和SITSC[261]促进了作物分类和覆盖的发展。相应的，Emelyanova等[273]，Zeebruges[274]和WUDAPT[275]促进了数据融合的发展。这些数据集为时间分析开创了先例。</p>
<p><strong>Cross-element Dataset</strong>  跨元素数据集主要关注RS图像与其他类型数据的交互和转换，如图像/音频[30,72,244]、图像/文本[26,214,239]和图像/大气数据[262,276]。RS图像描述[214,239,277]、跨模态检索[26,30,72,244]和大气数据观测[262,276]都依赖于这些数据集。由于RS图像的规模大，包含的内容太多，其他类型的数据往往很难描述和对齐整个场景的关键信息。因此，当前跨元素数据集的主要问题仍然是如何将不同元素之间的信息以一种有效、同构的表示方式进行合理、详细的表示。</p>
<h2 id="9-Applications"><a href="#9-Applications" class="headerlink" title="9 Applications"></a>9 Applications</h2><p><img data-src="https://cdn.jsdelivr.net/gh/FouforPast/pic-storage@main/img/image-20221029204517475.png" alt="image-20221029204517475"></p>
<h4 id="9-1-Land-Use-Classification"><a href="#9-1-Land-Use-Classification" class="headerlink" title="9.1 Land Use Classification"></a>9.1 Land Use Classification</h4><p>土地利用分类(图10左上)是MRSII最早的应用。不同的土地覆被在卫星图像中具有相似的光谱特征，单一模态分类不可避免地会导致一些鉴别误差。通过多模态数据对其进行分析，可以从三个方面增强它们:1)分辨率;2)光谱;3)时间。MRSII有效地提高了同区域的分辨率，减少了混合像元的数量;高光谱分辨率提高了光谱维度信息的保真度和准确性;时间信息被不同时间序列中土地覆盖类型的不同特征进一步补充。</p>
<p>Chen等人[278]将Landsat-8数据与MODIS、HJ-1A和ASTER DEM数据融合，以提高土地覆盖分类精度。一项研究应用Sentinel-1、Sentinel-2和Landsat-8数据解决了由于云层覆盖导致的空间不连续的问题[279]。在[280]中，作者进一步研究了融合数据的不同级别(数据级、特征级和决策级)的效果比较。</p>
<h4 id="9-2-Urban-Planning"><a href="#9-2-Urban-Planning" class="headerlink" title="9.2 Urban Planning"></a>9.2 Urban Planning</h4><p>通过对多模态数据的分析，大大缩短了同一区域的观测间隔。因此，数据的多模态提供了多角度观察城市变化和发展的可能性，通过对历史数据的观察，可以有效地规划和预测城市的发展(图10右上)。</p>
<p>在[281]中，作者提出了一种基于两幅异构图像提出了一种无监督深度卷积耦合网络用于变化检测。最近的一项研究探索了使用卷积自编码器和通用自编码器来消除两个异构图像(光学和SAR)中的大部分冗余，以获得更一致的特征表示[282]。另一项研究设计了一个边缘保存神经网络(edge-preservation neural network, EPUNet)，它用极少的人工干预就可以将现有的建筑数据库自动更新到它们的最新状态[54]。</p>
<h4 id="9-3-Agriculture-and-Ecology"><a href="#9-3-Agriculture-and-Ecology" class="headerlink" title="9.3 Agriculture and Ecology"></a>9.3 Agriculture and Ecology</h4><p>多模态卫星图像的监测在农业和生态两个方向都具有重大的政治和经济意义(图10底部)。许多作物往往在同一时刻外观相似，需要通过卫星是时间序列图像进行观测，以提高分类精度。在生态学中，多模态图像在生态变量估计、生态系统动态监测和生态系统干扰检测等方面也有很大的应用潜力[283,284]。</p>
<p>Garnot等人[22,23]提出使用基于自注意力机制的定制神经体系结构提取时相特征，并为大规模农业地块分类设计了轻量级的时间自注意力。一项研究通过整合多时相和多光谱遥感数据，研究了一种用于大规模动态玉米和大豆制图的DeepCropMapping方法[285]。He等人[286]结合细颗粒物(PM2.5)浓度、地表温度(LST)和植被覆盖(VC)的遥感数据，在国家的尺度、城市群之间和快速城市化地区评估了城市环境变化。Hilker等[287]和Tran等[288]使用STAARCH融合Landsat和MODIS反射率数据来绘制森林扰动图。</p>
<h2 id="10-Future-Directions"><a href="#10-Future-Directions" class="headerlink" title="10 Future Directions"></a>10 Future Directions</h2><p>随着数据源的增加，MRSII提供了高分辨率、高光谱和长时间观测的可行性。同时，它也给遥感领域带来了更多的任务和挑战。下面，我们从不同的角度提出了一些潜在的研究方向。</p>
<p><strong>Multimodal Image Restoration</strong>  由于多模态图像恢复的各种有趣应用，它已经吸引了越来越多的研究人员的兴趣。与单模态图像恢复不同，该任务更倾向于从异构图像中获取互补信息进行图像恢复，这需要对不同模态之间的依赖关系进行适当建模。该领域在去噪任务如去除云层[204,289,290]中具有非常重要的作用。</p>
<p><strong>3D Scene Reconstruction and Multi-view Interpretation</strong>  从卫星图像进行场景模型的自动三维重建仍然是一个具有挑战性的研究课题。该方向有许多有趣的应用，如场景建模、城市仿真和路径规划。在复杂遥感场景建模时，需要从多个角度观察场景，同时涉及到各种数据源的分析。与室内自然场景重建相比，大尺度遥感场景更加复杂(特别是在复杂的城市地区)，因此带来了很大的挑战。</p>
<p>这一课题最近几年才出现，Huang等人[291,292]构建了一系列相关数据集，并将位姿估计方法应用到重构算法中，取得了很大的突破和进展。</p>
<p><strong>Land Use Classification and Detection</strong>  尽管近年来MRSII在土地利用分类和检测方面取得了相当大的进展，但由于以往的数据集往往不具有代表性，很难在所有实际场景中取得实效。目前，<strong>大多数方法缺乏鲁棒性和通用性，而且它们都是针对特定类别和数据集进行了过度设计，削弱了对其他更通用场景的适用性</strong>。一个理想的多模态解码框架应该能够处理具有不同数据复杂度和数据源的各种学习任务。因此，如何提高方法的鲁棒性和通用性是当前任务的热点课题。</p>
<p><strong>Heterogeneous Image Time Series Change Detection</strong>  目前，<strong>异构图像变化检测任务只考虑双时相遥感图像</strong>。而在实际应用中，我们往往需要通过对一系列长时间序列图像的分析，来推断出场景在该时间段内的变化和发展，这对于城市发展、规划和自然环境保护都是非常有用的。</p>
<p><strong>Scene Prediction and Complementary</strong>  场景预测与互补是一个新兴的研究方向。<strong>它通过对场景长时间序列的特征提取和建模，预测场景的未来发展或补充中间时刻的元素</strong>。这项任务为区域发展预测和历史分析提供了可能性。</p>
<p><strong>Cross-element analysis</strong>  由于遥感图像规模大、更为复杂，对跨元素分析提出了很大的挑战。该任务的关键是提取复杂场景中的关键实例，并将它们与其他模态对齐或转换。因此，本课题主要涉及多源对齐(第3节)和跨模态翻译(第6节)相关内容，主要涉及遥感图像-语音(文本)对齐、遥感场景描述、遥感场景问答等研究方向。</p>
<h2 id="11-Conclusion"><a href="#11-Conclusion" class="headerlink" title="11 Conclusion"></a>11 Conclusion</h2><p>利用多源数据进行大规模场景观测和解译是遥感和计算机视觉领域进一步发展的关键。据我们所知，本文是第一个描述多模态遥感领域进展的综述，并提出了一个简明易懂的分类法来对所有MRSII方法进行分组。通过深入分析，从空间、时间、跨元素三个方向分析了MRSII方法，揭示了主流方法之间的内在联系。MRSII最近成为一个活跃的研究领域；因此，我们希望这项调查可以帮助研究人员，作为一个起点，回顾最新的发展，并为他们提供一个系统的和前所未有的概述。</p>

      <div class="tags">
          <a href="/tags/%E9%81%A5%E6%84%9F/" rel="tag"><i class="ic i-tag"></i> 遥感</a>
          <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"><i class="ic i-tag"></i> 多模态</a>
          <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag"><i class="ic i-tag"></i> 论文阅读</a>
      </div>
  </div>

   <footer>

    <div class="meta">
  <span class="item">
    <span class="icon">
      <i class="ic i-calendar-check"></i>
    </span>
    <span class="text">更新于</span>
    <time title="修改时间：2024-04-08 15:49:57" itemprop="dateModified" datetime="2024-04-08T15:49:57+08:00">2024-04-08</time>
  </span>
  <span id="2022/10/29/遥感多模态综述/" class="item leancloud_visitors" data-flag-title="遥感多模态综述" title="阅读次数">
      <span class="icon">
        <i class="ic i-eye"></i>
      </span>
      <span class="text">阅读次数</span>
      <span class="leancloud-visitors-count"></span>
      <span class="text">次</span>
  </span>
</div>

      

<div id="copyright">
<ul>
  <li class="author">
    <strong>本文作者： </strong>Foufor Past <i class="ic i-at"><em>@</em></i>FP 的一亩三分地
  </li>
  <li class="link">
    <strong>本文链接：</strong>
    <a href="https://fouforpast.github.io/2022/10/29/%E9%81%A5%E6%84%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%BC%E8%BF%B0/" title="遥感多模态综述">https://fouforpast.github.io/2022/10/29/遥感多模态综述/</a>
  </li>
  <li class="license">
    <strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

  </footer>

</article>

  </div>
  

<div class="post-nav">
    <div class="item left">
      

  <a href="/2021/12/31/KMP%E7%AE%97%E6%B3%95/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipey0a334j20zk0m8qpt.jpg" title="KMP算法">
  <span class="type">上一篇</span>
  <span class="category"><i class="ic i-flag"></i> 算法学习</span>
  <h3>KMP算法</h3>
  </a>

    </div>
    <div class="item right">
      

  <a href="/2022/11/03/n%E4%B8%AA%E7%90%83%E6%94%BE%E5%85%A5m%E4%B8%AA%E7%AE%B1%E5%AD%90%E9%97%AE%E9%A2%98/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipeybxm1pj20zk0m8niv.jpg" title="n个球放入m个箱子问题">
  <span class="type">下一篇</span>
  <span class="category"><i class="ic i-flag"></i> 算法学习</span>
  <h3>n个球放入m个箱子问题</h3>
  </a>

    </div>
</div>

  
  <div class="wrap" id="comments"></div>


        </div>
        <div id="sidebar">
          

<div class="inner">

  <div class="panels">
    <div class="inner">
      <div class="contents panel pjax" data-title="文章目录">
          <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#From-Single-to-Multi-modal-Remote-Sensing-Imagery-Interpretation-A-Survey-and-Taxonomy"><span class="toc-number">1.</span> <span class="toc-text">From Single- to Multi-modal Remote Sensing Imagery Interpretation: A Survey and Taxonomy</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Taxonomy"><span class="toc-number">1.3.</span> <span class="toc-text">2. Taxonomy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Multi-source-Alignment"><span class="toc-number">1.4.</span> <span class="toc-text">3. Multi-source Alignment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Spatial-Alignment"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">3.1 Spatial Alignment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Temporal-Alignment"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">3.2 Temporal Alignment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-Cross-element-Alignment"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">3.3 Cross-element Alignment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-Related-work-and-Challenges"><span class="toc-number">1.4.0.4.</span> <span class="toc-text">3.4 Related work and Challenges</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Muti-source-Fusion"><span class="toc-number">1.5.</span> <span class="toc-text">4. Muti-source Fusion</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-Homogeneous-Data-Fusion"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">4.1 Homogeneous Data Fusion</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-1-Spatial-reference"><span class="toc-number">1.5.0.1.1.</span> <span class="toc-text">4.1.1 Spatial reference</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-2-Spatio-temporal-reference"><span class="toc-number">1.5.0.1.2.</span> <span class="toc-text">4.1.2 Spatio-temporal reference</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-Heterogeneous-Data-Fusion"><span class="toc-number">1.5.0.2.</span> <span class="toc-text">4.2 Heterogeneous Data Fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-Remote-Sensing-And-Other-Type-Data-Fusion"><span class="toc-number">1.5.0.3.</span> <span class="toc-text">4.3 Remote Sensing And Other Type Data Fusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-Remaining-problems"><span class="toc-number">1.5.0.4.</span> <span class="toc-text">4.4 Remaining problems</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Multimodal-Representation"><span class="toc-number">1.6.</span> <span class="toc-text">5 Multimodal Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-Joint-Representation"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">5.1 Joint Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-Coordinated-Representation"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">5.2 Coordinated Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-Encoder-decoder-Representation"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">5.3 Encoder-decoder Representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-Discussion"><span class="toc-number">1.6.0.4.</span> <span class="toc-text">5.4 Discussion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Cross-modal-Translation"><span class="toc-number">1.7.</span> <span class="toc-text">6. Cross-modal Translation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-Cross-sensor-translation"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">6.1 Cross-sensor translation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-Cross-element-Translation"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">6.2 Cross-element Translation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-The-challenges-and-differences-from-nature-scenes"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">6.3 The challenges and differences from nature scenes</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Co-learning"><span class="toc-number">1.8.</span> <span class="toc-text">7. Co-learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Datasets-of-MRSII"><span class="toc-number">1.9.</span> <span class="toc-text">8. Datasets of MRSII</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Applications"><span class="toc-number">1.10.</span> <span class="toc-text">9 Applications</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-Land-Use-Classification"><span class="toc-number">1.10.0.1.</span> <span class="toc-text">9.1 Land Use Classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-2-Urban-Planning"><span class="toc-number">1.10.0.2.</span> <span class="toc-text">9.2 Urban Planning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-3-Agriculture-and-Ecology"><span class="toc-number">1.10.0.3.</span> <span class="toc-text">9.3 Agriculture and Ecology</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Future-Directions"><span class="toc-number">1.11.</span> <span class="toc-text">10 Future Directions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Conclusion"><span class="toc-number">1.12.</span> <span class="toc-text">11 Conclusion</span></a></li></ol></li></ol>
      </div>
      <div class="related panel pjax" data-title="系列文章">
        <ul>
          <li class="active"><a href="/2022/10/29/%E9%81%A5%E6%84%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%BC%E8%BF%B0/" rel="bookmark" title="遥感多模态综述">遥感多模态综述</a></li><li><a href="/2024/04/08/multimodal-remote-sensing-imagery-classification/" rel="bookmark" title="More diverse means better, multimodal deep learning meets remote-sensing imagery classification论文笔记">More diverse means better, multimodal deep learning meets remote-sensing imagery classification论文笔记</a></li>
        </ul>
      </div>
      <div class="overview panel" data-title="站点概览">
        <div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="image" itemprop="image" alt="Foufor Past"
      data-src="/images/avatar.jpg">
  <p class="name" itemprop="name">Foufor Past</p>
  <div class="description" itemprop="description"></div>
</div>

<nav class="state">
    <div class="item posts">
      <a href="/archives/">
        <span class="count">32</span>
        <span class="name">文章</span>
      </a>
    </div>
    <div class="item categories">
      <a href="/categories/">
        <span class="count">2</span>
        <span class="name">分类</span>
      </a>
    </div>
    <div class="item tags">
      <a href="/tags/">
        <span class="count">25</span>
        <span class="name">标签</span>
      </a>
    </div>
</nav>

<div class="social">
</div>

<ul class="menu">
  
    
  <li class="item">
    <a href="/" rel="section"><i class="ic i-home"></i>首页</a>
  </li>

    
  <li class="item">
    <a href="/about/" rel="section"><i class="ic i-user"></i>关于</a>
  </li>

        
  <li class="item dropdown">
      <a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a>
    <ul class="submenu">

        
  <li class="item">
    <a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a>
  </li>

        
  <li class="item">
    <a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a>
  </li>

        
  <li class="item">
    <a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a>
  </li>

  </ul>

</ul>

      </div>
    </div>
  </div>

  <ul id="quick">
    <li class="prev pjax">
        <a href="/2021/12/31/KMP%E7%AE%97%E6%B3%95/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a>
    </li>
    <li class="up"><i class="ic i-arrow-up"></i></li>
    <li class="down"><i class="ic i-arrow-down"></i></li>
    <li class="next pjax">
        <a href="/2022/11/03/n%E4%B8%AA%E7%90%83%E6%94%BE%E5%85%A5m%E4%B8%AA%E7%AE%B1%E5%AD%90%E9%97%AE%E9%A2%98/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a>
    </li>
    <li class="percent"></li>
  </ul>
</div>


        </div>
        <div class="dimmer"></div>
      </div>
    </main>
    <footer id="footer">
      <div class="inner">
        <div class="widgets">
          

        </div>
        <div class="status">
  <div class="copyright">
    
    &copy; 2010 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="ic i-sakura rotate"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Foufor Past @ Foufor Past</span>
  </div>
  <div class="powered-by">
    基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span>
  </div>
</div>

      </div>
    </footer>
  </div>
<script data-config type="text/javascript">
  var LOCAL = {
    path: '2022/10/29/遥感多模态综述/',
    favicon: {
      show: "（●´3｀●）Goooood",
      hide: "(´Д｀)Booooom"
    },
    search : {
      placeholder: "文章搜索",
      empty: "关于 「 ${query} 」，什么也没搜到",
      stats: "${time} ms 内找到 ${hits} 条结果"
    },
    valine: true,fancybox: true,
    copyright: '复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',
    ignores : [
      function(uri) {
        return uri.includes('#');
      },
      function(uri) {
        return new RegExp(LOCAL.path+"$").test(uri);
      }
    ]
  };
</script>

<script src="https://cdn.polyfill.io/v2/polyfill.js"></script>

<script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script>

<script src="/js/app.js?v=0.2.5"></script>




<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
